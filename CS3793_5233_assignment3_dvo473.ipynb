{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS3793_5233_assignment3_dvo473.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelchapa/ai_machineLearning_assign3/blob/master/CS3793_5233_assignment3_dvo473.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wUL_Fy5qUDI",
        "colab_type": "text"
      },
      "source": [
        "# UTSA CS 3793/5233: Assignment-3\n",
        "\n",
        "Summer 2020\n",
        "\n",
        "\n",
        "**Chapa - Michael - (dvo473)**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM8b9KVYsETT",
        "colab_type": "text"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "Implement 2 different machine learning algorithms\n",
        "*   Stochastic Gradient Descent\n",
        "*   ID3 Decision Tree\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzR4Ic34zJlT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Description\n",
        "\n",
        "This assignment is focused on **machine learning**, mainly on the implementation of 2 different algorithms - Stochastic Gradient Descent & ID3 decision tree. \n",
        "The assignment is divided into two sections, each for one unique ML algorithm. \n",
        "\n",
        "The base structure and comments are provided on what should be done. You can use some libraries that help support you for the successful completion of the assignment. However, you **CANNOT** use a complete library that contains the implementation of ML algorithms. You can get pieces of code from online, but please cite the source properly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnPfmHAOteOI",
        "colab_type": "text"
      },
      "source": [
        "##Import Libraries\n",
        "\n",
        "Write all the import statements here. This should be for both algorithm implmentations. As mentioned before, you can not use any premade ML libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9apbZGptej6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import all required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeYRnesWqvLm",
        "colab_type": "text"
      },
      "source": [
        "#Stochastic Gradient Descent\n",
        "\n",
        "In this section, you will implement the Stochastic Gradient Descent algorithm. The training is for a **binary classification** task i.e. each instance will have a class value of 0 or 1. Also, assume that you are given **all binary-valued attributes** and that there are **no missing values** in the train or test data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUVZIK6ctMi4",
        "colab_type": "text"
      },
      "source": [
        "##Algorithm\n",
        "\n",
        "(40 points)\n",
        "\n",
        "Following are the data files that will be provided to you for the gradient descent algorithm implementation.\n",
        "\n",
        "*   Training file - 'gd-train.dat'\n",
        "*   Testing file - 'gd-test.dat'\n",
        "\n",
        "*Both these files should be present in the same folder as this code file.* In these files, only non-space characters are relevant. The first line contains the attribute names. All the other lines are different example instances to be used for the algorithm. Each column holds values of the attributes, whereas the last column holds the class label for that instance.\n",
        "\n",
        "Write the code in the following code block, structure is provided. Instructions on the steps to follow are provided as comments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdPwgSBOtb1P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "619cc810-542c-490f-8c48-ce2d08892cb1"
      },
      "source": [
        "# utility fxn, determines if prediction is a 1 or 0\n",
        "def threshold(y_pred):\n",
        "    if y_pred >= 0.5:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "# Load data\n",
        "trainingData = np.loadtxt(\"https://raw.githubusercontent.com/michaelchapa/\" \\\n",
        "                          \"ai_machineLearning_assign3/master/gd-train.dat\", skiprows = 1)\n",
        "testData = np.loadtxt(\"https://raw.githubusercontent.com/michaelchapa/\" \\\n",
        "                      \"ai_machineLearning_assign3/master/gd-test.dat\", skiprows = 1)\n",
        "\n",
        "rows, attributes = trainingData.shape # attributes = columns\n",
        "rows2, attributes2 = testData.shape\n",
        "\n",
        "# Get all Learning Rates, from 0.05 to 1 (inclusive)\n",
        "alphas = []\n",
        "alpha = 0.05\n",
        "while alpha <= 1.05:\n",
        "    alphas.append(alpha)\n",
        "    alpha += 0.05\n",
        "\n",
        "# Learn for each learning rate, print results, do it again\n",
        "for alpha in alphas:\n",
        "    weights = np.zeros(attributes - 1) # weights initialized to 0\n",
        "    \n",
        "    # Train the weights\n",
        "    for instance in trainingData: # each row in the data\n",
        "        y_pred = np.dot(weights, instance[0:-1]) # make y prediction\n",
        "        error = y_pred - instance[-1]\n",
        "        \n",
        "        # update each weight\n",
        "        for j in range(0, 13): \n",
        "            weights[j] = weights[j] - alpha * error * instance[j]\n",
        "    \n",
        "    # Check accuracy on trainingData\n",
        "    correctCount = 0\n",
        "    for instance in trainingData:\n",
        "        y_pred = np.dot(weights, instance[0:-1]) # predict\n",
        "        y_pred = threshold(y_pred)\n",
        "        \n",
        "        if y_pred == instance[-1]:\n",
        "            correctCount += 1\n",
        "        \n",
        "    accuracy = (correctCount / rows) * 100.0\n",
        "    print(\"Accuracy for LR of %.2lf on Training set = %.0lf\" \\\n",
        "          % (alpha, accuracy), end = \"%\\n\")\n",
        "    \n",
        "    # Check accuracy on testData\n",
        "    correctCount = 0\n",
        "    for instance in testData:\n",
        "        y_pred = np.dot(weights, instance[0:-1])\n",
        "        y_pred = threshold(y_pred)\n",
        "        \n",
        "        if y_pred == instance[-1]:\n",
        "            correctCount += 1\n",
        "            \n",
        "    accuracy = (correctCount / rows2) * 100.0\n",
        "    print(\"Accuracy for LR of %.2lf on Testing set =  %.0lf\" \\\n",
        "          % (alpha, accuracy), end = \"%\\n\")\n",
        "        \n",
        "    print()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for LR of 0.05 on Training set = 69%\n",
            "Accuracy for LR of 0.05 on Testing set =  71%\n",
            "\n",
            "Accuracy for LR of 0.10 on Training set = 68%\n",
            "Accuracy for LR of 0.10 on Testing set =  70%\n",
            "\n",
            "Accuracy for LR of 0.15 on Training set = 71%\n",
            "Accuracy for LR of 0.15 on Testing set =  67%\n",
            "\n",
            "Accuracy for LR of 0.20 on Training set = 68%\n",
            "Accuracy for LR of 0.20 on Testing set =  64%\n",
            "\n",
            "Accuracy for LR of 0.25 on Training set = 69%\n",
            "Accuracy for LR of 0.25 on Testing set =  70%\n",
            "\n",
            "Accuracy for LR of 0.30 on Training set = 68%\n",
            "Accuracy for LR of 0.30 on Testing set =  72%\n",
            "\n",
            "Accuracy for LR of 0.35 on Training set = 35%\n",
            "Accuracy for LR of 0.35 on Testing set =  35%\n",
            "\n",
            "Accuracy for LR of 0.40 on Training set = 70%\n",
            "Accuracy for LR of 0.40 on Testing set =  67%\n",
            "\n",
            "Accuracy for LR of 0.45 on Training set = 32%\n",
            "Accuracy for LR of 0.45 on Testing set =  28%\n",
            "\n",
            "Accuracy for LR of 0.50 on Training set = 32%\n",
            "Accuracy for LR of 0.50 on Testing set =  28%\n",
            "\n",
            "Accuracy for LR of 0.55 on Training set = 32%\n",
            "Accuracy for LR of 0.55 on Testing set =  28%\n",
            "\n",
            "Accuracy for LR of 0.60 on Training set = 32%\n",
            "Accuracy for LR of 0.60 on Testing set =  28%\n",
            "\n",
            "Accuracy for LR of 0.65 on Training set = 32%\n",
            "Accuracy for LR of 0.65 on Testing set =  28%\n",
            "\n",
            "Accuracy for LR of 0.70 on Training set = 32%\n",
            "Accuracy for LR of 0.70 on Testing set =  28%\n",
            "\n",
            "Accuracy for LR of 0.75 on Training set = 32%\n",
            "Accuracy for LR of 0.75 on Testing set =  28%\n",
            "\n",
            "Accuracy for LR of 0.80 on Training set = 68%\n",
            "Accuracy for LR of 0.80 on Testing set =  72%\n",
            "\n",
            "Accuracy for LR of 0.85 on Training set = 68%\n",
            "Accuracy for LR of 0.85 on Testing set =  72%\n",
            "\n",
            "Accuracy for LR of 0.90 on Training set = 32%\n",
            "Accuracy for LR of 0.90 on Testing set =  28%\n",
            "\n",
            "Accuracy for LR of 0.95 on Training set = 68%\n",
            "Accuracy for LR of 0.95 on Testing set =  72%\n",
            "\n",
            "Accuracy for LR of 1.00 on Training set = 67%\n",
            "Accuracy for LR of 1.00 on Testing set =  71%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYxmgcnes9cS",
        "colab_type": "text"
      },
      "source": [
        "##Extra Credit - Accuracy Plots\n",
        "\n",
        "(5 points)\n",
        "\n",
        "Use the above accuracy results on the training and testing data and write code to plot the graphs as mentioned in the code block below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbBNakSDq0Wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the graphs for accuracy results.\n",
        "# There will be 2 graphs - one for training data and the other for testing data\n",
        "# For each graph,\n",
        "    # X-axis will be the learning rate going from 0.05-1 in increments on 0.05\n",
        "    # Y-axis will be the accuracy values at the selected learning rate.\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onnqJYTEq0l3",
        "colab_type": "text"
      },
      "source": [
        "#ID3 Decision Tree\n",
        "\n",
        "In this section, you will implement the ID3 Decision Tree algorithm. The training is for a **binary classification** task i.e. each instance will have a class value of 0 or 1. Also, assume that you are given **all binary-valued attributes** and that there are **no missing values** in the train or test data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDNztBkTtRPw",
        "colab_type": "text"
      },
      "source": [
        "##Algorithm\n",
        "\n",
        "(80 points)\n",
        "\n",
        "Following are the data files that will be provided to you for the ID3 algorithm implementation.\n",
        "\n",
        "*   Training file - 'id3-train.dat'\n",
        "*   Testing file - 'id3-test.dat'\n",
        "\n",
        "*Both these files should be present in the same folder as this code file.* In these files, only non-space characters are relevant. The first line contains the attribute names. All the other lines are example instances to be used for the algorithm. Each column holds values of the attributes, whereas the last column holds the class label for that instance.\n",
        "\n",
        "In a decision tree, if you reach a leaf node but still have examples that belong to different classes, then choose the most frequent class (among the instances at the leaf node). If you reach a leaf node in the decision tree and have no examples left or the examples are equally split among multiple classes, then choose the class that is most frequent in the entire training set. You do not need to implement pruning. Also, donâ€™t forget to use logarithm base 2 when computing entropy and set (0 log 0) to 0.\n",
        "\n",
        "Write the code in the following code block, structure is provided. Instructions on the steps to follow are provided as comments. The code should output the following 3 things:\n",
        "\n",
        "*   Print the Decision Tree created, in the following example format:\n",
        "\n",
        "    ```\n",
        "    attr1 = 0 :\n",
        "        attr2 = 0 :\n",
        "            attr3 = 0 : 1\n",
        "            attr3 = 1 : 0\n",
        "        attr3 = 1 :\n",
        "            attr4 = 0 : 0\n",
        "            attr4 = 1 : 1\n",
        "    attr1 = 1 :\n",
        "        attr2 = 1 : 1\n",
        "\n",
        "    ```\n",
        "\n",
        "*   Accuracy on the Training data = x %\n",
        "*   Accuracy on the Test data = x %\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XELGzRDftS77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "1bf823fe-9608-43c4-c3bb-7e4e1923dd97"
      },
      "source": [
        "\n",
        "# returns tuple of avg/conditional entropy of given feature & class to split on\n",
        "def Entropy_Feature(feature, target):\n",
        "    zz = zo = oz = oo = 0\n",
        "    \n",
        "    zipped = zip(feature, target)\n",
        "    for tuple in zipped:\n",
        "        if tuple == (0.0, 0.0):\n",
        "            zz += 1\n",
        "        elif tuple == (0.0, 1.0): # y -> we play\n",
        "            zo += 1\n",
        "        elif tuple == (1.0, 0.0):\n",
        "            oz += 1\n",
        "        elif tuple == (1.0, 1.0): # x -> we play \n",
        "            oo += 1\n",
        "    \n",
        "    ztotal = zo + zz\n",
        "    ototal = oo + oz\n",
        "    \n",
        "    entropy1 = ((-1 * (oo/ ototal)) * math.log((oo/ ototal), 2)) + \\\n",
        "        ((-1 * (oz/ ototal)) * math.log((oz/ ototal), 2)) \n",
        "        \n",
        "    entropy0 = ((-1 * (zo/ ztotal)) * math.log((zo/ ztotal), 2)) + \\\n",
        "        ((-1 * (zz/ ztotal)) * math.log((zz/ ztotal), 2)) \n",
        "        \n",
        "    averageEntropy1 = entropy1 * (ototal / (ototal + ztotal))\n",
        "    averageEntropy0 = entropy0 * (ztotal / (ototal + ztotal))\n",
        "    averageEntropy = averageEntropy1 + averageEntropy0\n",
        "    \n",
        "    if averageEntropy1 < averageEntropy0:\n",
        "        return averageEntropy, 1\n",
        "    else:\n",
        "        return averageEntropy, 0\n",
        "\n",
        "def Entropy_Target(target):\n",
        "    hits = target.sum()\n",
        "    total = len(target)\n",
        "    ratio = hits/ total\n",
        "    antiRatio = (total - hits) / total\n",
        "    return ((-1 * ratio) * math.log(ratio, 2)) + \\\n",
        "        ((-1 * antiRatio) * math.log(antiRatio, 2))\n",
        "\n",
        "def Information_Gain(entropies):\n",
        "    # get min entropy from list\n",
        "    minEntropy, route = min((x, y) for x, y in entropies)\n",
        "    minEntropyIndex = [x[0] for x in entropies].index(minEntropy)\n",
        "    \n",
        "    return minEntropyIndex, route\n",
        "            \n",
        "def RemoveColumn(index, data):\n",
        "    # remove column based on index\n",
        "    df = pd.DataFrame(data = data)\n",
        "    df = df.drop(index, axis = 1) # remove column\n",
        "    return df.to_numpy() # convert back\n",
        "\n",
        "def ID3(data, tabs):\n",
        "    rows, columns = np.shape(data)\n",
        "    target = trainData.T[-1]\n",
        "    \n",
        "    if columns == 2: # Feature Column left\n",
        "        target = trainData.T[-1]\n",
        "        features_avg_entropy = [Entropy_Feature(feature, target) for feature in data.T[:-1]]\n",
        "        target_entropy = Entropy_Target(target) \n",
        "        index, route = Information_Gain(features_avg_entropy)\n",
        "        print(\"\\t\" * tabs, end = \"\")\n",
        "        print(\"Split at column index: \" + str(index) + \", value: \" + str(1))\n",
        "        print(\"\\t\" * tabs, end = \"\")\n",
        "        print(\"Split at column index: \" + str(index) + \", value: \" + str(0))\n",
        "        # print(\"\\t\", features_avg_entropy)\n",
        "        data = RemoveColumn(index, data)\n",
        "        return\n",
        "    else:\n",
        "        target = trainData.T[-1]\n",
        "        features_avg_entropy = [Entropy_Feature(feature, target) for feature in data.T[:-1]]\n",
        "        target_entropy = Entropy_Target(target) \n",
        "        index, route = Information_Gain(features_avg_entropy)\n",
        "        print(\"\\t\" * tabs, end = \"\")\n",
        "        print(\"Split at column index: \" + str(index) + \", value: \" + str(route))\n",
        "        data = RemoveColumn(index, data)\n",
        "        ID3(data, tabs + 1) \n",
        "    \n",
        "trainData = np.loadtxt(\"https://raw.githubusercontent.com/michaelchapa\" \\\n",
        "                        \"/ai_machineLearning_assign3/master/id3-train.dat\", skiprows = 1)\n",
        "testData = np.loadtxt(\"https://raw.githubusercontent.com/michaelchapa\" \\\n",
        "                       \"/ai_machineLearning_assign3/master/id3-test.dat\", skiprows = 1)\n",
        "\n",
        "ID3(trainData, 0)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split at column index: 4, value: 0\n",
            "\tSplit at column index: 3, value: 0\n",
            "\t\tSplit at column index: 0, value: 1\n",
            "\t\t\tSplit at column index: 2, value: 1\n",
            "\t\t\t\tSplit at column index: 1, value: 0\n",
            "\t\t\t\t\tSplit at column index: 0, value: 1\n",
            "\t\t\t\t\tSplit at column index: 0, value: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvYowzzA4vcd",
        "colab_type": "text"
      },
      "source": [
        "##Extra Credit - Learning Curve\n",
        "\n",
        "(10 points)\n",
        "\n",
        "Instead of taking the entire training data (all 800 instances), loop through to select 'x' instances in the increments of 40 (i.e. 40, 80, 120, and so on). For each selected number 'x', randomly pick the example instances from the training data and call the ID3 function to create the decision tree. Calculate the accuracy of the created ID3 tree on the Test data file. Plot the corresponding graph, aka Learning Curve.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYSK99zp5a7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loop through to select the number of instances 'x' in increments of 40\n",
        "# For each 'x',\n",
        "    # Randomly select 'x' instances\n",
        "    # Create the ID3 decision tree using those instances\n",
        "    # Calculate the accuracy of the ID3 tree created on the Test data\n",
        "\n",
        "# Plot the learning curve using the accuracy values\n",
        "    # X-axis will be the number of training instances used for creating the tree\n",
        "    # Y-axis will be the accuracy in % on the Test data\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJSFgNBQrhQU",
        "colab_type": "text"
      },
      "source": [
        "#Submission Instructions\n",
        "\n",
        "\n",
        "\n",
        "1.   Complete all tasks above.\n",
        "2.   Export this notebook as .ipynb\n",
        "      (File > Download as ipynb)\n",
        "3.   Upload the .ipynb file on Blackboard\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lGvLE9H6ptL",
        "colab_type": "text"
      },
      "source": [
        "##Rubric\n",
        "\n",
        "*   (40 points) Gradient Descent Algorithm\n",
        "*   (05 points) Extra Credit - GD Accuracy Plots\n",
        "*   (80 points) ID3 Algorithm\n",
        "*   (10 points) Extra Credit - ID3 Learning Curve\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}